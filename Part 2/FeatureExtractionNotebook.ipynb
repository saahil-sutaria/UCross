{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary packages\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup constants\n",
    "DATASET_DIRECTORY = 'C:/Users/thoma/Documents/CSU East Bay/2nd Year/Fall 2019/CS 663/Projects/Project 3/Video Dataset (Full)'\n",
    "VIDEO_PATH = os.path.join(DATASET_DIRECTORY, '**', '*.mp4')\n",
    "IMAGE_SHAPE = 224\n",
    "SEQUENCE_LENGTH = 40\n",
    "BATCH_SIZE = 10\n",
    "MIN_CLASS_SAMPLES = 5\n",
    "TRAINING_PERCENTAGE = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator function that will get frames from a video file and prepare them for feature \n",
    "# extraction with a MobileNetV2 CNN\n",
    "def frame_generator():\n",
    "    video_paths = tf.io.gfile.glob(VIDEO_PATH)\n",
    "    np.random.shuffle(video_paths)\n",
    "    for video_path in video_paths:\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        sample_every_frame = max(1, num_frames // SEQUENCE_LENGTH)\n",
    "        current_frame = 0\n",
    "        \n",
    "        label = os.path.basename(os.path.dirname(video_path))\n",
    "        \n",
    "        max_images = SEQUENCE_LENGTH\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            \n",
    "            # OpenCV reads in videos in BGR format so we need to rearrange the channels\n",
    "            # to be in RGB format, resize the image, and preprocess it for the CNN\n",
    "            if current_frame % sample_every_frame == 0:\n",
    "                frame = frame[:, :, ::-1]\n",
    "                img = tf.image.resize(frame, (IMAGE_SHAPE, IMAGE_SHAPE))\n",
    "                img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "                max_images -= 1\n",
    "                yield img, video_path\n",
    "                \n",
    "            if max_images == 0:\n",
    "                break\n",
    "            current_frame += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset from the generator function to yield preprocessed images\n",
    "# and the files that they came from\n",
    "dataset = tf.data.Dataset.from_generator(frame_generator,\n",
    "                                         output_types=(tf.float32, tf.string),\n",
    "                                         output_shapes=((IMAGE_SHAPE, IMAGE_SHAPE, 3), ()))\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a MobileNetV2 model to use as the feature extraction model\n",
    "mobilenet_v2 = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(IMAGE_SHAPE,IMAGE_SHAPE,3), \n",
    "                                                              include_top=False, weights='imagenet')\n",
    "CNN_output = mobilenet_v2.output\n",
    "pooling_output = tf.keras.layers.GlobalAveragePooling2D()(CNN_output)\n",
    "feature_extraction_model = tf.keras.Model(mobilenet_v2.input,pooling_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1479it [21:06,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "current_path = None\n",
    "all_features = []\n",
    "\n",
    "# go through the dataset and use the MobileNetV2 model to \n",
    "# extract the features (1x1280) for each desired frame of \n",
    "# the videos\n",
    "for img, batch_paths in tqdm.tqdm(dataset):\n",
    "    # extract the features\n",
    "    batch_features = feature_extraction_model(img)\n",
    "    \n",
    "    # reshape the tensor\n",
    "    batch_features = tf.reshape(batch_features,(batch_features.shape[0], -1))\n",
    "    \n",
    "    for features, path in zip(batch_features.numpy(), batch_paths.numpy()):\n",
    "        # save the features corresponding to a video file when a new video file is reached\n",
    "        if path != current_path and current_path is not None:\n",
    "            output_path = current_path.decode().replace('.mp4','.npy')\n",
    "            np.save(output_path, all_features)\n",
    "            all_features = []\n",
    "            \n",
    "        current_path = path\n",
    "        all_features.append(features)\n",
    "    \n",
    "# save the final file after exiting the for loop\n",
    "output_path = current_path.decode().replace('.mp4','.npy')\n",
    "np.save(output_path, all_features)\n",
    "all_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all of the subdirectories (1 per class) in the dataset directory\n",
    "sub_dirs = []\n",
    "for root,dirs,files in os.walk(DATASET_DIRECTORY):\n",
    "    sub_dirs.append(dirs)\n",
    "\n",
    "sub_dirs = sub_dirs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create .txt files containing all of the video feature files that will be \n",
    "# used for model training, validation, and testing\n",
    "test_filename = os.path.join(DATASET_DIRECTORY,'TestList.txt')\n",
    "train_filename = os.path.join(DATASET_DIRECTORY,'TrainList.txt')\n",
    "validation_filename = os.path.join(DATASET_DIRECTORY,'ValidateList.txt')\n",
    "test_file = open(test_filename, 'w+')\n",
    "train_file = open(train_filename, 'w+')\n",
    "validation_file = open(validation_filename, 'w+')\n",
    "\n",
    "# loop through all of the data for the different classes\n",
    "for i in range(len(sub_dirs)):\n",
    "    path = os.path.join(DATASET_DIRECTORY,sub_dirs[i],'*.npy')\n",
    "    features_path = tf.io.gfile.glob(path)\n",
    "    np.random.shuffle(features_path)\n",
    "    num_files = len(features_path)\n",
    "    \n",
    "    # if the class has the required minimum number of samples\n",
    "    if num_files > MIN_CLASS_SAMPLES:\n",
    "        # write the testing files (2 per class)\n",
    "        test_file.write(features_path[0] + '\\n')\n",
    "        test_file.write(features_path[1] + '\\n')\n",
    "        num_files = num_files - 2\n",
    "        train_size = math.floor(num_files*TRAINING_PERCENTAGE)\n",
    "        validate_size = num_files-train_size\n",
    "        \n",
    "        # write the training files (TRAINING_PERCENTAGE of the dataset minus the 2 training files per class)\n",
    "        for i in range(train_size):\n",
    "            train_file.write(features_path[2+i] + '\\n')\n",
    "        \n",
    "        # write the validation files (1-TRAINING_PERCENTAGE of the dataset minus the 2 training files per class)\n",
    "        for i in range(validate_size):\n",
    "            validation_file.write(features_path[2+train_size+i] + '\\n')\n",
    "                \n",
    "test_file.close()\n",
    "train_file.close()\n",
    "validation_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
