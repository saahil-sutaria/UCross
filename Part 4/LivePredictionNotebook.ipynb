{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary packages\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the constants\n",
    "IMG_SHAPE  = 224  # size of our input image needed for our model IMG_SHAPE x IMG_SHAPE x 3 (color)\n",
    "BASE_DIRECTORY = 'C:/Users/thoma/Documents/CSU East Bay/2nd Year/Fall 2019/CS 663/Projects/Project 3/Part 1'\n",
    "URL = 'http://192.168.0.9:8081'\n",
    "USE_WEBCAM = False\n",
    "\n",
    "# labels for the 3 classes\n",
    "LABELS = ['MoveLeft','MoveRight','MoveStraight']\n",
    "\n",
    "DECISION_DIFFERENCE_THRESHOLD = 0.1\n",
    "SEQUENCE_LENGTH = 40\n",
    "FEATURE_LENGTH = 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load a model from an h5 file\n",
    "def loadModelFrom_H5_File(model_file):\n",
    "    new_model = tf.keras.models.load_model(model_file)\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a feature extraction model (MobileNetV2 CNN)\n",
    "def CreateCNN(image_shape):\n",
    "    mobilenet_v2 = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(image_shape,image_shape,3), \n",
    "                                                                  include_top=False, weights='imagenet')\n",
    "    cnn_output = mobilenet_v2.output\n",
    "    pooling_output = tf.keras.layers.GlobalAveragePooling2D()(cnn_output)\n",
    "    feature_extraction_model = tf.keras.Model(mobilenet_v2.input,pooling_output)\n",
    "    \n",
    "    return feature_extraction_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to re-size and pre-process an image from the camera and return a tensor\n",
    "def ProcessImage(image,image_size):\n",
    "    img = tf.image.resize(image, (image_size,image_size))\n",
    "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "    img = np.expand_dims(img, axis=0)          \n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract all of the features from a list of properly sized images\n",
    "def ExtractFeatures(image_array,feature_extraction_model):\n",
    "    all_features = []\n",
    "    image_array = np.asarray(image_array)\n",
    "    \n",
    "    # loop through all the images in the image_array list and extract their features\n",
    "    for i in range(image_array.shape[0]):\n",
    "        features = feature_extraction_model(image_array[i])\n",
    "        features = tf.reshape(features,(features.shape[0], -1))\n",
    "        features = features.numpy()\n",
    "        all_features.append(features)\n",
    "    \n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes a list of feature vectors and returns a properly formatted numpy array for prediction\n",
    "def PrepareFeatures(features,sequence_length,feature_size):\n",
    "    features = np.asarray(features)\n",
    "    features = np.reshape(features,(1,sequence_length,feature_size))\n",
    "    padded_sequence = np.zeros((1,sequence_length,feature_size))\n",
    "    padded_sequence[0:feature_size] = np.array(features)\n",
    "    \n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to use an LSTM model to make a prediction on live video data\n",
    "def predict(input, model):\n",
    "    prediction = model.predict(input, batch_size=1, verbose=0)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make a classification decision and return a label\n",
    "def makeDecision(predictions, class_labels):\n",
    "    max_index = np.argmax(predictions)\n",
    "    predictions2 = np.delete(predictions,[max_index],None)\n",
    "    max_index2 = np.argmax(predictions2)\n",
    "    \n",
    "    if(max_index2 >= max_index):\n",
    "        max_index2 = max_index2 + 1\n",
    "    \n",
    "    if(predictions[0][max_index] >= predictions[0][max_index2] and \n",
    "      (predictions[0][max_index]-predictions[0][max_index2]) > DECISION_DIFFERENCE_THRESHOLD):\n",
    "        label = class_labels[max_index]\n",
    "    else:\n",
    "        label = \"Unknown\"\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a folder to store prediction info in the specified parent directory\n",
    "def CreatePredictionFolder(parent_dir,classification):\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    prediction_directory = classification + '_live_' + timestr\n",
    "    prediction_directory = os.path.join(parent_dir,prediction_directory)\n",
    "    os.mkdir(prediction_directory)\n",
    "    \n",
    "    return prediction_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes a list of BGR frames and saves them to the prediction_folder\n",
    "# the frames must be in BGR format as OpenCV will rearrange the channels during saving\n",
    "# so that each saved frame will be in RGB format\n",
    "def SavePredictionFrames(all_BGR_frames,prediction_folder):    \n",
    "    all_frames = np.asarray(all_BGR_frames)\n",
    "    for i in range(all_frames.shape[0]):\n",
    "        img_file = 'Image_' + str(i+1) + '.jpg'\n",
    "        img_file = os.path.join(prediction_folder,img_file)\n",
    "        frame = all_frames[i]\n",
    "        cv2.imwrite(img_file,frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a .txt file for all of the feature vectors used to make a prediction\n",
    "def SaveVideoFeatures(prediction_dir,features):\n",
    "    filename = os.path.join(prediction_dir,'FeatureVectors.txt')\n",
    "    np.savetxt(filename,features[0],delimiter = ', ',newline = '\\n\\n\\n',\n",
    "               header = '\\n\\n', footer = '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a .txt file for all of the prediction results (label and prediction vector)\n",
    "def SavePredictionResults(prediction_dir,prediction_results,classification):\n",
    "    filename = os.path.join(prediction_dir,'PredictionResults.txt')\n",
    "    np.savetxt(filename,prediction_results,delimiter = ', ',newline = '\\n\\n',\n",
    "               header = '\\n\\nPrediction: ' + classification + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "# load the saved model and create the MobileNetV2 CNN\n",
    "model_files = os.path.join(BASE_DIRECTORY, '*.h5')\n",
    "model_paths = tf.io.gfile.glob(model_files)\n",
    "model_file = model_paths[0]\n",
    "model = loadModelFrom_H5_File(model_file)   \n",
    "mobilenet = CreateCNN(IMG_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a folder to store all of the live capture predictions if it doesn't already exist\n",
    "results_directory = os.path.join(BASE_DIRECTORY,'LiveCaptureResults')\n",
    "if not os.path.exists(results_directory):\n",
    "    os.mkdir(results_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.1) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-d2ba2a3f391c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# capture a frame from the video capture stream and adjust its size and channel format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mis_capturing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframeorig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mBGR_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframeorig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIMG_SHAPE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIMG_SHAPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minterpolation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINTER_LINEAR\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# resize BGR frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mRGB_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBGR_frame\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# convert BGR to RGB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.1.1) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "# use either a webcam or a phone acting as an IP camera to capture video\n",
    "if USE_WEBCAM:\n",
    "    vc = cv2.VideoCapture(0)\n",
    "    \n",
    "else:\n",
    "    vc = cv2.VideoCapture(URL)\n",
    "\n",
    "current_frame = 0\n",
    "max_images = SEQUENCE_LENGTH\n",
    "all_images = []\n",
    "all_BGR_frames = []\n",
    "\n",
    "if vc.isOpened(): # try to get the first frame\n",
    "    is_capturing, frame = vc.read()    #read frame\n",
    "    \n",
    "else:\n",
    "    is_capturing = False\n",
    "\n",
    "# continue executing as long as frames can be captured and an interrupt doesn't occur\n",
    "while is_capturing:\n",
    "    try:    \n",
    "        \n",
    "        # capture a frame from the video capture stream and adjust its size and channel format        \n",
    "        is_capturing, frameorig = vc.read()\n",
    "        BGR_frame = cv2.resize(frameorig,(IMG_SHAPE,IMG_SHAPE),interpolation = cv2.INTER_LINEAR)  # resize BGR frame\n",
    "        RGB_frame = cv2.cvtColor(BGR_frame,cv2.COLOR_BGR2RGB)    # convert BGR to RGB\n",
    "        \n",
    "        all_BGR_frames.append(BGR_frame)\n",
    "        \n",
    "        # pre-process image into a tensor and convert the underlying data from type uint8 to float32\n",
    "        image = ProcessImage(RGB_frame,IMG_SHAPE)\n",
    "        all_images.append(image)\n",
    "        max_images -= 1           \n",
    "        \n",
    "        # after getting all SEQUENCE_LENGTH frames\n",
    "        if max_images == 0:           \n",
    "            all_features = ExtractFeatures(all_images,mobilenet)\n",
    "            prediction_input = PrepareFeatures(all_features,SEQUENCE_LENGTH,FEATURE_LENGTH)\n",
    "            prediction = predict(prediction_input, model)\n",
    "            classification = makeDecision(prediction,LABELS)\n",
    "            \n",
    "            # make folder to store each separate prediction\n",
    "            prediction_directory = CreatePredictionFolder(results_directory,classification)\n",
    "            \n",
    "            # create file to store the prediction results and the feature vectors \n",
    "            SaveVideoFeatures(prediction_directory,prediction_input)\n",
    "            SavePredictionResults(prediction_directory,prediction,classification)\n",
    "            \n",
    "            # save all of the SEQUENCE_LENGTH frames used to make the prediction\n",
    "            SavePredictionFrames(all_BGR_frames,prediction_directory)          \n",
    "            current_frame = 0 \n",
    "            all_images = []\n",
    "            all_BGR_frames = []\n",
    "            max_images = SEQUENCE_LENGTH\n",
    "            \n",
    "        current_frame += 1\n",
    "                               \n",
    "    except KeyboardInterrupt:\n",
    "        vc.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
